# TG-018 — AI plumbing (RAG + Chat) — Profile A (Supabase + pgvector)

**Goal**  
Introduce a minimal, provider-agnostic AI layer for Tiny Greenhouse:
- RAG retrieval over our crop knowledge (YAML/Markdown seed files)
- Chat endpoint that grounds answers on retrieved chunks + current greenhouse snapshot
- No UI changes in this ticket

Keep the task **scope-only** (no big code samples). Follow `backend/AGENTS.md` conventions.

---

## What we’re building (concise)
- **Vector store:** Supabase Postgres with `pgvector` (Free tier).  
- **Embeddings:**
  - **Documents:** precomputed offline during seeding (CLI).
  - **Queries:** OpenAI `text-embedding-3-small` (cheap) — switchable later.
- **LLM provider abstraction:** default **OpenAI** (gpt-4o-mini), with env switch for future **Ollama**.
- **Retrieval:** top-k (k=8) by cosine, filtered by `crop_id`, `lang`, and optional `growth_stage`.
- **Grounding:** merge RAG chunks + current config + latest telemetry snapshot into one assistant answer.

---

## Deliverables
1) **Seed pack format** (docs only): place source files under `data/rag/`:
   - `data/rag/crops/<crop>/<variety>.yaml` — growth stages, defaults, warnings, quick FAQ.
   - Optional `.md` companions for longer explanations.
   - Language: **EN** for now.
2) **CLI seeder** (skeleton functionality):
   - Reads the seed pack, chunks content, computes **document embeddings**, upserts into `rag_chunks` table.
   - Command: `npm run rag:seed` (documented in README).
3) **Vector store wiring (pgvector)**:
   - Table `rag_chunks` with columns: id, crop_id, stage, lang, source_path, chunk, embedding (vector).
   - Suitable indexes for vector search + basic filters.
4) **Embedding/LLM adapters** (thin, env-driven):
   - `EMBED_PROVIDER=openai` (default); room for `local` later.
   - `LLM_PROVIDER=openai` (default); room for `ollama` later.
5) **Endpoints (backend only)**:
   - `POST /api/rag/search` — DEV ONLY, returns retrieved chunks (for debugging).
   - `POST /api/assist` — accepts `{ message }`, resolves `crop_id/lang/stage` from the user’s greenhouse, performs retrieval, returns grounded answer.
   - `GET /api/health/ai` — verifies connectivity to vector DB + providers.
6) **Policies**:
   - Rate limit (e.g., 30 req/hour per user) and simple input size guard.
   - No secrets in frontend; all keys in backend `.env`.

---

## Env & Config (document only, no secrets)
- `VECTOR_STORE=pgvector`
- `DATABASE_URL=<supabase-postgres-url>`
- `OPENAI_API_KEY=***` (for query embeddings + chat)
- `EMBED_PROVIDER=openai`
- `LLM_PROVIDER=openai`
- Optional: `RAG_TOP_K=8`

Deployment note: Supabase Free with `pgvector` is sufficient for this demo (thousands of chunks).

---

## Acceptance criteria
- `npm run rag:seed` loads **at least one** crop/variety file and persists chunks with embeddings in the DB.
- `POST /api/rag/search` returns relevant chunks when filtered by active crop (dev only; behind a guard).
- `POST /api/assist` returns a concise answer that references only retrieved sources; if insufficient data → a clear „not enough data“ message.
- `GET /api/health/ai` confirms: DB reachable, embeddings callable, and LLM provider responding.
- No new UI; server-only changes. No secrets in client code.

---

## Guardrails for the assistant (doc text for system prompt)
- Use **only** the provided SOURCES (retrieved chunks) and SNAPSHOT (telemetry + config + growth stage).
- If the answer isn’t supported by SOURCES, clearly say you don’t have enough data and suggest next steps.
- Prefer concise, actionable responses; include short rationales when changing defaults or warning about risks.

---

## Out of scope (for future tickets)
- Azure Vision integration (Image Analysis F0) — separate ticket.
- Multilingual answers beyond EN (we’ll add BG later).
- UI for `/assistant`.
